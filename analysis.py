import re
import pandas as pd
from collections import Counter
from tqdm import tqdm
import spacy
from string import punctuation

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É
def load_text_file(file_path):
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ç–µ–∫—Å—Ç–æ–≤–∏–π —Ñ–∞–π–ª.
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ —Ä—è–¥–∫—ñ–≤ –∑ —Ñ–∞–π–ª–∞.
    """
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return [line.strip() for line in lines if line.strip()]

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç—É –≤—ñ–¥ –µ–º–æ–¥–∑—ñ, –ø–æ—Å–∏–ª–∞–Ω—å —ñ –∑–∞–π–≤–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤
def clean_text(text):
    """
    –û—á–∏—â—É—î —Ç–µ–∫—Å—Ç –≤—ñ–¥ –µ–º–æ–¥–∑—ñ, –ø–æ—Å–∏–ª–∞–Ω—å, –∑–∞–π–≤–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤ —ñ –ø—Ä–æ–±—ñ–ª—ñ–≤.
    """
    emoji_pattern = re.compile(
        "[" 
        u"\U0001F600-\U0001F64F"  # –µ–º–æ—Ü—ñ—ó
        u"\U0001F300-\U0001F5FF"  # —Å–∏–º–≤–æ–ª–∏ —ñ –ø—ñ–∫—Ç–æ–≥—Ä–∞–º–∏
        u"\U0001F680-\U0001F6FF"  # —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç
        u"\U0001F700-\U0001F77F"  # —ñ–Ω—à—ñ —Å–∏–º–≤–æ–ª–∏
        u"\U0001F780-\U0001F7FF"  # –≥–µ–æ–º–µ—Ç—Ä–∏—á–Ω—ñ —Ñ–æ—Ä–º–∏
        u"\U0001F800-\U0001F8FF"  # —Ä—ñ–∑–Ω–µ
        u"\U0001F900-\U0001F9FF"  # –∂–µ—Å—Ç–∏ —ñ –æ–±'—î–∫—Ç–∏
        u"\U0001FA00-\U0001FA6F"  # –º–µ–¥–∏—á–Ω—ñ —Å–∏–º–≤–æ–ª–∏
        u"\U00002700-\U000027BF"  # —Ä—ñ–∑–Ω–µ
        "]+", flags=re.UNICODE
    )
    # –ü—Ä–∏–±–∏—Ä–∞—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
    url_pattern = re.compile(r'http[s]?://\S+|www\.\S+')

    # –†–µ–≥—É–ª—è—Ä–Ω—ñ –≤–∏—Ä–∞–∑–∏
    text = emoji_pattern.sub('', text)  # –ø—Ä–∏–±–∏—Ä–∞—î–º–æ –µ–º–æ–¥–∑—ñ
    text = url_pattern.sub('', text)   # –ø—Ä–∏–±–∏—Ä–∞—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
    text = re.sub(r'\s+', ' ', text).strip()  # –ø—Ä–∏–±–∏—Ä–∞—î–º–æ –∑–∞–π–≤—ñ –ø—Ä–æ–º—ñ–∂–∫–∏
    return text

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –∑ –≤–∏–∫–ª—é—á–µ–Ω–Ω—è–º —Å—Ç–æ–ø-—Å–ª—ñ–≤ —ñ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—ó
# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –∑ –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫–æ–º —Ä–µ—á–µ–Ω—å —ñ —Ç–æ–∫–µ–Ω—ñ–≤
def analyze_texts(texts, nlp):
    """
    –ê–Ω–∞–ª—ñ–∑—É—î —Ç–µ–∫—Å—Ç–∏: —Ä–∞—Ö—É—î —Å–∏–º–≤–æ–ª–∏, —Å–ª–æ–≤–∞, —É–Ω—ñ–∫–∞–ª—å–Ω—ñ —Å–ª–æ–≤–∞,
    —Ä–µ—á–µ–Ω–Ω—è —ñ —Ç–æ–∫–µ–Ω–∏. –í–∏–∫–ª—é—á–∞—î —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ —ñ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é –ø—Ä–∏ –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—ñ.
    """
    print("\nüìù –ü—Ä–æ–≤–æ–¥–∏—Ç—å—Å—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑...")
    all_text = ' '.join(texts)

    # –ó–±—ñ–ª—å—à—É—î–º–æ –ª—ñ–º—ñ—Ç –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —Ç–µ–∫—Å—Ç—É, —è–∫—â–æ —Ç–µ–∫—Å—Ç –¥—É–∂–µ –≤–µ–ª–∏–∫–∏–π
    nlp.max_length = len(all_text) + 1000

    # –î—ñ–ª–∏–º–æ —Ç–µ–∫—Å—Ç –Ω–∞ –±–ª–æ–∫–∏ –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—à–æ—ó –æ–±—Ä–æ–±–∫–∏ –≤–µ–ª–∏–∫–∏—Ö —Ñ–∞–π–ª—ñ–≤
    block_size = 100000  # –†–æ–∑–º—ñ—Ä –±–ª–æ–∫—É (—É —Å–∏–º–≤–æ–ª–∞—Ö)
    blocks = [all_text[i:i+block_size] for i in range(0, len(all_text), block_size)]

    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª—ñ—á–∏–ª—å–Ω–∏–∫—ñ–≤
    total_sentences = 0  # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ—á–µ–Ω—å
    total_tokens = 0     # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤
    filtered_words = []  # –°–ø–∏—Å–æ–∫ –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–∏—Ö —Å–ª—ñ–≤

    # –ê–Ω–∞–ª—ñ–∑ –∫–æ–∂–Ω–æ–≥–æ –±–ª–æ–∫—É —Ç–µ–∫—Å—Ç—É
    for block in tqdm(blocks, desc="–û–±—Ä–æ–±–∫–∞ –±–ª–æ–∫—ñ–≤"):
        doc = nlp(block)
        total_sentences += len(list(doc.sents))  # –†–∞—Ö—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ—á–µ–Ω—å
        total_tokens += len(doc)  # –†–∞—Ö—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤
        filtered_words.extend([
            token.text.lower() for token in doc 
            if not token.is_stop and token.text not in punctuation and token.is_alpha
        ])

    # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—ñ —Å–ª—ñ–≤
    word_freq = Counter(filtered_words).most_common(10)

    # –ó–∞–≥–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ
    total_chars = len(all_text)  # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–∏–º–≤–æ–ª—ñ–≤
    total_words = len(all_text.split())  # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤
    unique_words = len(set(all_text.split()))  # –£–Ω—ñ–∫–∞–ª—å–Ω—ñ —Å–ª–æ–≤–∞

    # –í–∏–≤–µ–¥–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print(f"üìä –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
#   print(f"    üî∏ –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–∏–º–≤–æ–ª—ñ–≤: {total_chars}")
#   print(f"    üî∏ –ö—ñ–ª—å–∫—ñ—Å—Ç—å —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å–ª—ñ–≤: {unique_words}")
    print(f"    üî∏ –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤: {total_tokens}")
    print(f"    üî∏ –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤: {total_words}")
    print(f"    üî∏ –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ—á–µ–Ω—å: {total_sentences}")
#   print(f"    üî∏ –¢–æ–ø-10 –Ω–∞–π—á–∞—Å—Ç—ñ—à–∏—Ö —Å–ª—ñ–≤ (–±–µ–∑ —Å—Ç–æ–ø-—Å–ª—ñ–≤ —ñ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—ó): {word_freq}")

# –û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è
def main(input_file, output_cleaned_file, linguistic_model):
    print("üöÄ –ü–æ—á–∞—Ç–æ–∫ –æ–±—Ä–æ–±–∫–∏...")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É
    texts = load_text_file(input_file)
    print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(texts)} —Ä—è–¥–∫—ñ–≤.")

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ SpaCy
    print("üîç –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ SpaCy...")
    nlp = spacy.load(linguistic_model)
    print("‚úÖ –ú–æ–¥–µ–ª—å SpaCy –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")

    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç—É
    print("üßπ –û—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É...")
    cleaned_texts = [clean_text(text) for text in tqdm(texts, desc="–û—á–∏—â–µ–Ω–Ω—è")]

    # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–æ—Ä–æ–∂–Ω—ñ—Ö —Ä—è–¥–∫—ñ–≤
    cleaned_texts = [text for text in cleaned_texts if text.strip()]
    print(f"‚úÖ –ü—ñ—Å–ª—è –æ—á–∏—Å—Ç–∫–∏ –∑–∞–ª–∏—à–∏–ª–æ—Å—è {len(cleaned_texts)} —Ä—è–¥–∫—ñ–≤.")

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –æ—á–∏—â–µ–Ω–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤
    with open(output_cleaned_file, 'w', encoding='utf-8') as file:
        file.write('\n'.join(cleaned_texts))
    print(f"‚úÖ –û—á–∏—â–µ–Ω—ñ —Ç–µ–∫—Å—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª: {output_cleaned_file}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
    analyze_texts(cleaned_texts, nlp)

# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
input_file = 'prepared_texts.txt'  # –í—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª (–ª–∏—à–µ —Ç–µ–∫—Å—Ç–∏)
output_cleaned_file = 'cleaned_texts.txt'  # –§–∞–π–ª –∑ –æ—á–∏—â–µ–Ω–∏–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏
linguistic_model = 'uk_core_news_sm'  # –ú–æ–¥–µ–ª—å SpaCy –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏

# –í–∏–∫–æ–Ω–∞–Ω–Ω—è
main(input_file, output_cleaned_file, linguistic_model)
